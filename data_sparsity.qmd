---
title: "Data Sparsity"
format: 
  html:
    theme: 
      - materia
      - custom.scss
    toc: true
    toc-location: left
knitr: 
  opts_chunk: 
    fig-width: 8
    fig.height: 5
editor: visual
author: Josef Fruehwald
license: CC-BY-SA 4.0
bibliography: references.bib
reference-location: margin
---

## Bug Catching

Let's say we're biologists, working in a rain forest, and put out a bug net to survey the biodiversity of the forest. We catch 10 bugs, and each species is a different color:

\[[{{< fa bug >}}$_1$]{.bug1}, [{{< fa bug >}}$_2$]{.bug1}, [{{< fa bug >}}$_3$]{.bug1}, [{{< fa bug >}}$_4$]{.bug1}, [{{< fa bug >}}$_5$]{.bug1}, [{{< fa bug >}}$_6$]{.bug2}, [{{< fa bug >}}$_7$]{.bug2}, [{{< fa bug >}}$_8$]{.bug3}, [{{< fa bug >}}$_9$]{.bug4}, [{{< fa bug >}}$_{10}$]{.bug5}\]

We have 10 bugs in total, so we'll say $N=10$. This is our "token count." We'll use the $i$ subscript to refer to each individual bug (or token).

If we made a table of each bug species, it would look like:

| species                 | index $j$ | count |
|-------------------------|-----------|-------|
| [{{< fa bug >}}]{.bug1} | 1         | 5     |
| [{{< fa bug >}}]{.bug2} | 2         | 2     |
| [{{< fa bug >}}]{.bug3} | 3         | 1     |
| [{{< fa bug >}}]{.bug4} | 4         | 1     |
| [{{< fa bug >}}]{.bug5} | 5         | 1     |

Let's use $M$ to represent the total number of species, so $M=5$ here. This is our *type* count, and we'll the subscript $j$ to represent the index of specific *types*.

We can mathematically represent the count of each species like so.

$$ 
c_j = C(\class{fa fa-bug}{}_j)
$$

Here, the function $C()$ takes a specific species representation $\class{fa fa-bug}{}_j$ as input, and returns the specific count $c_j$ for how many times that species showed up in our net. So when $j = {\color{#785EF0}{1}}$, $\color{#785EF0}{c_1}=5$, and when $j = {\color{#FFB000}{4}}$, $\color{#FFB000}{c_4}=1$.

Here's a plot, with the species id $j$ on the x-axis, and the number of times that species appeared in the net $c_j$ on the y-axis.

```{r}
#| include: false
library(tidyverse)
library(scales)
library(emojifont)
load.fontawesome()

library(showtext)

font_add_google("Roboto", "roboto")
showtext_auto()

theme_set(theme_minimal() + theme(text = element_text(family = "roboto", size = 16)))


library(gutenbergr)
library(tidytext)
library(ggrepel)
library(mgcv)
library(knitr)

library(qlcData)
library(ggplot2movies)
library(babynames)
```

```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 5
tribble(~species, ~count, 
        1, 5,
        2, 2,
        3, 1,
        4, 1,
        5, 1) %>%
  mutate(label = fontawesome("fa-bug")) %>%
  ggplot(aes(factor(species), count))+
    geom_text(label = fontawesome("fa-bug"),
              family='fontawesome-webfont',
              aes(color = factor(species)),
              size = 10) +
    scale_color_manual(values = c("#785EF0",
                                  "#DC267F",
                                  "#FE6100",
                                  "#FFB000",
                                  "#4C8C05"),
                       guide = "none")+
    ylim(0,5)+
    labs(x = expression(j), y = expression(paste(c[j])))+
    theme(axis.title.y = element_text(angle = 0, vjust = 0.5))
```

### Making Predictions

What is the probability that tomorrow, when we put the net out again, that the first bug we catch will be from species [{{< fa bug >}}]{.bug1}? Usually in these cases, we'll use past experience to predict the future. Today, of the $N=10$ bugs we caught, $\color{#785EF0}{c_1}=5$ of them were species [{{< fa bug >}}]{.bug1}. We can represent this as a fraction like so:

$$
\frac{{\color{#785EF0}{\class{fa fa-bug}{}}}_1, 
      {\color{#785EF0}{\class{fa fa-bug}{}}}_2,
      {\color{#785EF0}{\class{fa fa-bug}{}}}_3,
      {\color{#785EF0}{\class{fa fa-bug}{}}}_4,
      {\color{#785EF0}{\class{fa fa-bug}{}}}_5}
{{\color{#785EF0}{\class{fa fa-bug}{}}}_1, 
      {\color{#785EF0}{\class{fa fa-bug}{}}}_2,
      {\color{#785EF0}{\class{fa fa-bug}{}}}_3,
      {\color{#785EF0}{\class{fa fa-bug}{}}}_4,
      {\color{#785EF0}{\class{fa fa-bug}{}}}_5,
      {\color{#DC267F}{\class{fa fa-bug}{}}}_6,
      {\color{#DC267F}{\class{fa fa-bug}{}}}_7,
      {\color{#FE6100}{\class{fa fa-bug}{}}}_8,
      {\color{#FFB000}{\class{fa fa-bug}{}}}_9,
      {\color{#4C8C05}{\class{fa fa-bug}{}}}_{10}}
$$

Or, we can simplify it a little bit. The top part (the numerator) is equal to $\color{#785EF0}{c_1}=5$, and the bottom part (the denominator) is equal to the total number of bugs, $N$. Simplifying then:

$$
\frac{\color{#785EF0}{c_1}}{N} = \frac{5}{10} = 0.5
$$

We'll use this as our guesstimate of the probability that the very next bug we catch will be from species [{{< fa bug >}}]{.bug1}. Let's use the function $\hat{P}()$ to mean "our method for guessing the probability", and $\hat{p}$ to represent the guess we came to. We could express "our guess that the first bug we catch will be [{{< fa bug >}}]{.bug1}" like so.

$$
{\color{#785EF0}{\hat{p}_1}} = \hat{P}({\color{#785EF0}{\class{fa fa-bug}{}}}) = \frac{\color{#785EF0}{c_1}}{N} = \frac{5}{10} = 0.5
$$

We can then generalize our method to *any* bug like so:

$$
\hat{p}_j = \hat{P}(\class{fa fa-bug}{}_j) = \frac{c_j}{N}
$$

### A wild [{{< fa bug >}}]{.bug6} appeared!

Let's say we set out the net again, and the first bug we catch is actually [{{< fa bug >}}]{.bug6}. This is a new species of bug that wasn't in the net the first time. Makes enough sense, the forest is very large. However, what probability *would* we have given catching this new species?

Well, $\color{#35F448}{c_6} = C({\color{#35F448}{\class{fa fa-bug}{}}}) = 0$. So our estimate of the probability would have been ${\color{#35F448}{\hat{p}_6}} = \hat{P}({\color{#35F448}{\class{fa fa-bug}{}}}) = \frac{\color{#35F448}{c_6}}{N} = \frac{0}{10} = 0$.

Well obviously, the probability that we would catch a bug from species [{{< fa bug >}}]{.bug6} *wasn't* 0, because events with 0 probability don't happen, and we *did* catch the bug. Admittedly, $N=10$ is a small sample to try and base a probability estimate on, so how large *would* we need the sample to be before we could make probabity estimates for all possible bug species, assuming we stick with the probability estimating function $\hat{P}(\class{fa fa-bug}{}_j) = \frac{c_j}{N}$?

## You'd need {{< fa infinity >}}

This *kind* of data problem does arise for counting species, but this is really a tortured analogy for language data.[^1] For example, let's take all of the words from Chapter 1 of Mary Shelly's Frankenstein, downloaded from [Project Gutenberg](https://www.gutenberg.org/ebooks/84). I'll count how often each word occurred, and assign it a rank, with 1 being given to the word that occurred the most.

[^1]: For me, I used this analogy to include colorful images of bugs in the lecture notes. For @good1953, they had to use a tortured analogy since the methods for fixing probability estimates were still classified after being used to crack the Nazi Enigma Code in WWII.

```{r}
#| echo: false
#| eval: false
frank <- gutenberg_download(84)
```

```{r}
#| include: false

frank <- read_csv("frankenstein.csv")
```

```{r}
#| echo: false
frank %>%
  mutate(line = 1:n()) %>%
  filter(line >= 42) %>%
  mutate(section = case_when(grepl("^Letter \\d+$", text) ~ text,
                             grepl("^Chapter \\d+$", text) ~ text)) %>%
  fill(section) %>%
  filter(text != section) %>%
  unnest_tokens(word, text) %>%
  mutate(word = str_replace_all(word, "_", ""))-> frank_words
```

```{r}
#| echo: false
frank_words %>%
  filter(section == "Chapter 1") %>%
  count(word) %>%
  arrange(-n) %>%
  mutate(rank = 1:n()) -> frank_ch1

frank_ch1_5 <- frank_ch1%>% slice(1:10)

ch1_total <- sum(frank_ch1$n)
```

Just to draw the parallels between the two analogies:

| variable | in the analogy                                                | in Frankenstein Chapter 1                                                     |
|----------|---------------------------------------------------------------|-------------------------------------------------------------------------------|
| $N$      | The total number of bugs caught in the net. ($N=10$)          | The total number of words in the first chapter. ($N=1,780$).                  |
| $x_i$    | An individual bug. e.g. [{{< fa bug >}}]{.bug1}$_1$           | An individual word token. In chapter 1, $x_1$ = "i"                           |
| $w_j$    | A bug species. [{{< fa bug >}}]{.bug1}                        | A word type. The indices are frequency ordered, so for chapter 1 $w_1$ = "of" |
| $c_j$    | The count of how many *individuals* there are of a *species*. | The count of how many *tokens* there are of a *type*.                         |

Here's a table of the top 10 most frequent word types.

```{r}
#| echo: false

frank_ch1 %>%
  slice(1:10)%>%
  kable(col.names = c("$w_j$", "$c_j$", "$j$"))
```

If we plot out *all* of the word types with the rank ($j$) on the x-axis and the count of each word type ($c_j$) on the y-axis, we get a pattern that if you're not already familiar with it, you will be.

```{r}
#| echo: false
frank_ch1 %>%
  ggplot(aes(rank, n))+
    geom_point(aes(color = n == 1))+
    geom_text_repel(data = frank_ch1_5, 
                    aes(label = word),
                    nudge_x = 1)+
    scale_color_manual(values = c("black", "firebrick"))+
    labs(title = paste0("Frankenstein, Chapter 1: ", 
                        prettyNum(ch1_total,
                                  big.mark = ","), 
                        " words"),
         y = "count")+
    theme(axis.title.y = element_text(angle = 0, vjust = 0.5))
```

This is a "Zipfian Distribution" a.k.a. a "Pareto Distribution" a.k.a. a "Power law," and it has a few features which make it \~problematic\~ for all sorts of analyses.

For example, let's come back to the issue of predicting the probability of the next word we're going to see. Language Models are "string prediction models," after all, and in order to get a prediction for a specific string, you need to have *seen* the string in the training data. Remember how our bug prediction method had no way of predicting that we'd see a [{{< fa bug >}}]{.bug6} because it had never seen one before?

There are a lot of possible string types of "English" that we have not observed in Chapter 1 of Frankenstein. Good & Turing proposed that you could guesstimate that the probability of seeing a never before seen "species" was about equal to the proportion of "species" you'd only seen once. With just Chapter 1, that's a pretty high probability that there are words you haven't seen yet.

```{r}
#| echo: false
frank_ch1 %>%
  mutate(once = n == 1) %>%
  group_by(once) %>%
  summarise(total = sum(n)) %>%
  mutate(proportion = total/sum(total),
         once = ifelse(once, "yes", "no")) %>%
  kable(col.names = c("seen once?", "total", "proportion"),
        digits = 3)
```

So, let's increase our sample size. Here's the same plot of rank by count for chapters 1 through 5.

```{r}
#| echo: false
frank_words %>%
  filter(section %in% paste0("Chapter ", 1:5)) %>%
  count(word) %>%
  arrange(-n) %>%
  mutate(rank = 1:n()) -> frank_ch1to5

frank_ch1to5_5 <- frank_ch1to5%>% slice(1:10)

ch1to5_total <- sum(frank_ch1to5$n)

frank_ch1to5 %>%
  ggplot(aes(rank, n))+
    geom_point(aes(color = n == 1))+
    geom_text_repel(data = frank_ch1to5_5, 
                    aes(label = word),
                    nudge_x = 1)+
    scale_color_manual(values = c("black", "firebrick"))+
    scale_x_continuous(labels = label_comma())+
    scale_y_continuous(labels = label_comma())+
    labs(title = paste0("Frankenstein, Chapters 1 through 5: ", 
                        prettyNum(ch1to5_total,
                                  big.mark = ","), 
                        " words"))+
    theme(axis.title.y = element_text(angle = 0, vjust = 0.5))
```

```{r}
#| echo: false
frank_ch1to5 %>%
  mutate(once = n == 1) %>%
  group_by(once) %>%
  summarise(total = sum(n)) %>%
  mutate(proportion = total/sum(total),
         once = ifelse(once, "yes", "no")) %>%
  kable(col.names = c("seen once?", "total", "proportion"),
        digits = 3)
```

We increased the size of the whole corpus by a factor of 10, but we've still got a pretty high probability of encountering an unseen word.

Let's expand it out to the whole book now.

```{r}
#| echo: false
frank_words %>%
  count(word) %>%
  arrange(-n) %>%
  mutate(rank = 1:n()) -> frank_all

all_total = sum(frank_all$n)
all_top10 <- frank_all %>% slice(1:10)

frank_all %>%
  ggplot(aes(rank, n))+
    geom_point(aes(color = n == 1))+
    geom_text_repel(data = all_top10, 
                    aes(label = word),
                    nudge_x = 1)+
    scale_color_manual(values = c("black", "firebrick"))+
    scale_x_continuous(labels = label_comma())+
    scale_y_continuous(labels = label_comma())+
    labs(title = paste0("Frankenstein, Entire Book: ", 
                        prettyNum(all_total,
                                  big.mark = ","), 
                        " words"))+
    theme(axis.title.y = element_text(angle = 0, vjust = 0.5))
```

```{r}
#| echo: false
frank_all %>%
  mutate(once = n == 1) %>%
  group_by(once) %>%
  summarise(total = sum(n)) %>%
  mutate(proportion = total/sum(total),
         once = ifelse(once, "yes", "no")) %>%
  kable(col.names = c("seen once?", "total", "proportion"),
        digits = 3)
```

### 🎵 Ain't no corpus large enough 🎵

As it turns out, there's no corpus large enough to guarantee observing every possible word at least once, for a few reasons.

1.  The infinite generative capacity of language! The set of all possible words is, *in principle* infinitely large.
2.  These power law distributions will always have the a *lot* of tokens with a frequency of 1, and even just those tokens are going to have their probabilities poorly estimated.

To illustrate this, I downloaded the 1-grams of just words beginning with `[Aa]` from the [Google Ngrams data set](https://storage.googleapis.com/books/ngrams/books/datasetsv3.html). This is an ngram dataset based on all of the books scanned by the Google Books project. It's 4 columns wide, 86,618,505 rows long, and 1.8G large, and even then I think it's a truncated version of the data set, because the fewest number of years any given word appears is exactly 40.

```{r}
#| include: false
a_words <- read_tsv("~/Downloads/googlebooks-eng-all-1gram-20120701-a", 
                    col_names = F)
```

If we take just all of the words that start with `[Aa]` published in the year 2000, the most *common* frequency for a word to be is still just 1, even if it is a small proportion of all tokens.

```{r}
#| echo: false
#| tbl-cap: Frequencies of frequencies in words starting with `[Aa]` from the year 2000 in google ngrams
a_words %>%
  filter(X2 == 2000) %>%
  count(X3) %>%
  arrange(-n) %>%
  mutate(prop = X3/sum(X3 * n)) %>%
  head() %>%
  mutate(prop = format(prop, width = 5, digits = 3))%>%
  kable(col.names = c("word frequency", 
                      "number of types with frequency",
                      "proportion of all tokens"),
        align = c("r", "r","r"))
  
```

### An aside

I'll be plotting the rank vs the frequency with logarithmic axes from here on. Linear axes give equal visual space for every incremental change in the x and y values, while lograrithmic axes put more space between smaller numbers than larger numbers.

```{r}
#| echo: false
#| fig.width: 5
#| fig.height: 5
#| layout-ncol: 2
#| fig-subcap: 
#|   - rank by frequency on linear scales
#|   - rank by frequency on logarithmic scales
frank_all %>%
  ggplot(aes(rank, n))+
    geom_point()+
    geom_text_repel(data = all_top10, 
                    aes(label = word),
                    nudge_x = 1)+
    scale_x_continuous(labels = label_comma())+
    scale_y_continuous(labels = label_comma())

frank_all %>%
  ggplot(aes(rank, n))+
    geom_point()+
    geom_text_repel(data = all_top10, 
                    aes(label = word))+  
    scale_x_log10(labels = label_comma())+
    scale_y_log10(labels = label_comma())
```

### It gets worse

We can maybe get very far with our data sparsity for how often we'll see each individual word by increasing the size of our corpus size, but 1gram word counts are rarely as far as we'll want to go.

To come back to our bugs example, let's say that bug species [{{< fa bug >}}]{.bug6} actually hunts bug species [{{< fa bug >}}]{.bug4}. If we just caught a [{{< fa bug >}}]{.bug4} in our net, it's a lot more likely that we'll catch a [{{< fa bug >}}]{.bug6} next, coming after the helpless [{{< fa bug >}}]{.bug4} than it would be if we hadn't just caught a [{{< fa bug >}}]{.bug4}. To know what *exactly* the probability catching [{{< fa bug >}}]{.bug4} and then a [{{< fa bug >}}]{.bug6} is, we'd need to count up every 2 bug sequence we've seen.

Bringing this back to words, 2 word sequences are called "bigrams" and 3 word sequences are called "trigrams," and they are *also* distributed according to a Power Law, and each larger string of words has a worse data sparsity one than the one before. But each larger string of words means more context, which makes for better predictions.

```{r}
#| echo: false
frank_words %>%
  count(word) %>%
  arrange(-n) %>% 
  mutate(rank = 1:n()) -> frank_1gram

frank_1gram %>%
  slice(1:5)%>%
  mutate(label = word)-> frank_1gram_top

frank_words %>%
  mutate(next_word = lead(word)) %>%
  count(word, next_word) %>%
  arrange(-n) %>%
  mutate(rank = 1:n())->frank_bigram

frank_bigram %>%
  slice(1:5)%>%
  mutate(label = paste0(word, 
                        " ", 
                        next_word))-> frank_bigram_top

frank_words %>%
  mutate(next_word = lead(word),
         nnext_word = lead(next_word)) %>%
  count(word, next_word, nnext_word) %>%
  arrange(-n) %>%
  mutate(rank = 1:n())->frank_trigram

frank_trigram %>%
  slice(1:5)%>%
  mutate(label = paste0(word,
                        " ",
                        next_word,
                        " ",
                        nnext_word))-> frank_trigram_top

frank_1gram %>%
  ggplot(aes(rank, n))+
    geom_point(data = frank_trigram, 
               aes(color = "3gram"), 
               alpha= 0.6)+
    geom_text_repel(data = frank_trigram_top, aes(label = label,
                                                color = "3gram"),
                    show.legend  = FALSE)+
    geom_point(data = frank_bigram, 
               aes(color = "2gram"),
               alpha= 0.6)+
    geom_text_repel(data = frank_bigram_top, aes(label = label,
                                                color = "2gram"),
                    show.legend  = FALSE)+  
    geom_point(aes(color = "1gram"),
               alpha= 0.6)+
    geom_text_repel(data = frank_1gram_top, aes(label = label,
                                                color = "1gram"),
                    show.legend  = FALSE)+
    scale_x_log10(labels = label_comma())+
    scale_y_log10(labels = label_comma())+
    scale_color_viridis_d(name = "ngram",
                          end = 0.9,
                          option = "inferno")
```

## Some Notes on Power Laws

The power law distribution is pervasive in linguistic data, in almost every domain where we might count how often something happens or is observed. This is absolutely a fact that must be taken into account when we develop our theories or build our models. Some people also think it is an important fact to be explained about language, but I'm deeply skeptical.

A *lot* of things follow power law distributions. The general property of these distributions is that the second most frequent thing will have a frequency about as half as the most frequent thing, the third most frequent thing will have a frequency about a third of the most frequent thing, etc. We could put that mathematically as:

$$
c_j = \frac{c_1}{j}
$$

For example, here's the log-log plot of baby name rank by baby name frequency in the US between 1880 and 2017.[^2]

[^2]: Data from the `babynames` R package, which in turn got the data from the Social Security Administration.

```{r}
#| echo: false
#| fig-cap: rank by frequency of baby names
babynames %>%
  group_by(year) %>%
  arrange(-n) %>%
  mutate(rank = 1:n())%>%
  ggplot(aes(rank, n))+
    geom_line(aes(group = year, color = year))+
    scale_x_log10(labels = label_comma())+
    scale_y_log10(labels = label_comma())+
    scale_color_viridis_c(option = "plasma")+
    labs(y= "number of babies with name",
         title = "baby names")
```

The log-log plot isn't perfectly straight (it's common enough for data like this to have two "regimes").

Here's the number of ratings each movie on IMDB has received.

```{r}
#| echo: false
movies %>%
  arrange(-votes) %>%
  mutate(rank = 1:n()) -> movie_rank

movie_rank %>%
  slice(1:3) -> top_movies

movie_rank %>%
  ggplot(aes(rank, votes))+
     geom_point()+
     geom_text_repel(data = top_movies, 
                     aes(label = title),
                     size = 2)+
     scale_x_log10(labels = label_comma())+
     scale_y_log10(labels = label_comma())
```

If we break down the movies by their genre, we get the same kind of result.

```{r}
#| echo: false
movies %>%
  pivot_longer(cols = Action:Short, names_to = "genre", values_to = "coding") %>%
  filter(coding == 1) %>%
  group_by(genre) %>%
  arrange(-votes) %>%
  mutate(rank = 1:n()) %>%
  ggplot(aes(rank, votes, color = genre))+
    geom_line()+
    scale_x_log10(labels = label_comma())+
    scale_y_log10(labels = label_comma())+
    scale_color_brewer(palette = "Dark2")
```

Other things that have been shown to exhibit power law distributions [@newman2005; @jiang2011] are

-   US city populations
-   number of citations academic papers get
-   website traffic
-   number of copies books sell
-   earthquake magnitudes

These are all possibly examples of "preferential attachment", but we can also create an example that doesn't involve preferential attachment, and still wind up with a power-law. Let's take the first 12 words from Frankenstein:

````{=html}
<table>
<tr>
```{r}
#| echo: false
#| output: asis
for(i in 1:12){
  cat("<td>")
  cat("<code>")
  cat('"')
  cat(frank_words$word[i])
  cat('"')
  cat("</code>")
  cat("</td>")
}

```
</tr>
</table>
````

Now, let's paste them all together into one long string with spaces.

````{=html}
<table>
<tr>
```{r}
#| echo: false
#| output: asis
one_string <- str_c(frank_words$word[1:12], collapse = " ")
cat("<td>")
cat("<code>")
cat('"')
cat(one_string)
cat('"')
cat("</td>")
cat("</code>")

```
</tr>
</table>
````

And now, let's choose another arbitrary symbol to split up words besides `" "`. I'll go with `e`.

````{=html}
<table>
<tr>
```{r}
#| echo: false
#| output: asis
re_string <- str_split(one_string, pattern = "e")[[1]]
for(i in seq(along = re_string)){
  cat("<td>")
  cat("<code>")
  cat('"')  
  cat(re_string[i])
 cat('"')
  cat("</code>")
  cat("</td>")  
}
```
</tr>
</table>
````

The results *aren't* words. They're hardly useful substrings. But, if we do this to the entire novel and plot out the rank and count of thes substrings like they *were* words, we still get a power law distribution.

```{r}
#| echo: false
one_string <- str_c(frank_words$word, collapse = " ")
one_string <- str_replace_all(one_string, "\\s+", " ")
fake_words <- str_split(one_string, pattern = "e")
tibble(word = fake_words[[1]]) %>%
  count(word) %>%
  filter(nchar(word) > 0) %>%
  arrange(-n) %>%
  mutate(rank = 1:n(),
         len = nchar(word))->fake_count
fake_10 <- fake_count %>% slice(1:10)
fake_count %>%
  ggplot(aes(rank, n))+
    geom_point()+
    geom_text_repel(data = fake_10, aes(label = word))+
    scale_x_log10(labels = label_comma())+
    scale_y_log10(labels = label_comma())+
    labs(title = "substrings delimited by 'e'")
```

In fact, if I take the top 4 most frequent letters, besides spaces, that occur in the text and use them as substring delimiters, the resulting substring distributions are *all* power-law distributed.

```{r}
#| echo: false

delims <- names(sort(-table(str_split(one_string,pattern = ""))))[2:6]

split_frank <- function(delim, string){
  fake_words <- str_split(string, pattern = delim)
  tibble(word = fake_words[[1]],
         delim = delim) %>%
    filter(nchar(word) > 0) %>%
    count(word, delim) %>%
    arrange(-n) %>%
    mutate(rank = 1:n(),
          len = nchar(word))->fake_count
  return(fake_count)
}

fake_franks <- map_dfr(delims, ~split_frank(.x, one_string))

fake_franks %>%
  ggplot(aes(rank, n))+
    geom_text(aes(label = delim, color = delim),
              key_glyph = "rect")+
    scale_x_log10(labels = label_comma())+
    scale_y_log10(labels = label_comma())+
    scale_color_brewer(palette = "Dark2")

```

They even have other similar properties often associated with power law distributions in language. For example, it's often been noted that more frequent words tend to be shorter. These weird substrings exhibit that pattern even more strongly than actual words do!

```{r}
#| echo: false
#| message: false
#| error: false
frank_all <- frank_all %>%
  mutate(delim = " ", 
         len = nchar(word)) 
fake_franks %>%
  ggplot(aes(n, len))+
    stat_smooth(aes(color = delim), se = F)+
    stat_smooth(data = frank_all, aes(color = delim))+
    scale_x_log10(labels = label_comma())+
    scale_y_log10()+
    scale_color_brewer(palette = "Dark2")
```

This is all to say, be cautious about explanations for power-law distributions that are

## Extra

To work out just how accurate the Good-Turing estimate is, I did the following experiment.

Starting from the beginning of the book, I coded each word $w_i$ for whether or not it had already appeared in the book, 1 if yes, 0 if no. This is my best shot at writing that out in mathematical notation.

$$
a_i = \left\{\begin{array}{ll}1,& x_i\in x_{1:i-1}\\
                             0,& x_1 \notin x_{1:i-1}\end{array}\right\}
$$

Then for every position in the book, I made a table of counts of all the words up to that point in the book so far, and got the proportion of word tokens that had appeared only once. Again, here's my best stab at writing that out mathematically.

$$
c_{ji} = C(w_j), w_j \in x_{i:i-1}
$$

$$
r_i = \sum_{j=1}\left\{\begin{array}{ll}1,&c_{ji}=1\\0,& c_{ji} >1 \end{array}\right\}
$$

$$
g_i = \frac{r_i}{i-1}
$$

```{r}
#| eval: false
frank_words$first_appearance <- NA
frank_words$first_appearance[1] <- 1

frank_words$gt_est <- NA
frank_words$gt_est[1] <- 1
for(i in 2:nrow(frank_words)){
  i_minus <- i-1
  prev_corp <- frank_words$word[1:i_minus]
  this_word <- frank_words$word[i]
  
  frank_words$first_appearance[i] <- ifelse(this_word %in% prev_corp, 0, 1)
  frank_words$gt_est[i] <- sum(table(prev_corp) == 1)/i_minus
}
```

```{r}
#| echo: false
#| message: false
frank_words <- read_csv("frank_words.csv")
```

Then, I plotted the Good-Turing estimate for every position as well as a non-linear logistic regression smooth.

```{r}
#| echo: false
frank_words %>%
  mutate(id = 1:n()) %>%
  ggplot(aes(id, first_appearance))+
    geom_line(aes(y = gt_est,
                  color = "good-turing estimate"), 
              size = 0.5)+
    stat_smooth(method = gam,
                formula = y ~ s(x, bs = 'cs'),
                method.args = list(family = binomial),
                size = 0.5,
                aes(color = "logit estimate"))+
    scale_color_manual(name = NULL, values = c("black", "firebrick"))+
    scale_x_continuous(labels = label_comma())+
    labs(x = "word position in book",
         y = "probability of first appearance")+
    theme(legend.position = "top")
```
